"""Generate the JIF vs. consensus comparison-accuracy SI figure.

Source: 06_jif/04_jif_porc_accuracy.ipynb  (cell b6d5b8d7)

Target: _LATEX SOURCE/si_figs/pref_vs_jif_comparison_accuracy.pdf

Produces:
  - reproducibility/si_fig3.pdf

Data:
  - public_data/comparisons.csv                         → pairwise comparisons
  - public_data/venues.csv                              → jcr_jif per venue
  - reproducibility/derived/field_rankings_per_user.csv → per-user field SpringRank scores
  - reproducibility/derived/global_rankings_per_user.csv → per-user global (LOO) SpringRank scores

Notes:
  - field_rankings_per_user.csv:  SpringRank(alpha=20), field-level LOO
  - global_rankings_per_user.csv: SpringRank(alpha=20), global LOO (all fields pooled)
  - Both are generated by compute_rankings.py from public_data/comparisons.csv
  - Only non-tie comparisons where BOTH venues have a field rank AND a JIF are used.

Run from the project root:
  .venv/bin/python reproducibility/si_fig3.py
"""

from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

HERE   = Path(__file__).parent
ROOT   = HERE.parent
PUBLIC = ROOT / "public_data"
DERIVED = HERE / "derived"

# ── Load data ─────────────────────────────────────────────────────────────────
comp_df = pd.read_csv(PUBLIC / "comparisons.csv")

venues_df = pd.read_csv(PUBLIC / "venues.csv")
venue_citedness = dict(zip(venues_df["venue_db_id"], venues_df["jcr_jif"]))

field_rank_df  = pd.read_csv(DERIVED / "field_rankings_per_user.csv")
global_rank_df = pd.read_csv(DERIVED / "global_rankings_per_user.csv")

# ── Build per-user lookup dicts ───────────────────────────────────────────────
field_rankings = (
    field_rank_df.groupby("user_db_id")
    .apply(lambda g: dict(zip(g["venue_db_id"], g["score"])), include_groups=False)
    .to_dict()
)

global_rankings = (
    global_rank_df.groupby("user_db_id")
    .apply(lambda g: dict(zip(g["venue_db_id"], g["score"])), include_groups=False)
    .to_dict()
)

# ── Compute per-comparison accuracy ──────────────────────────────────────────
predictions = []

for _, row in comp_df.iterrows():
    if row["is_tie"]:
        continue

    user  = row["user_db_id"]
    pref  = row["pref_venue_db_id"]
    other = row["other_venue_db_id"]

    fr    = field_rankings.get(user, {})
    gr    = global_rankings.get(user, {})

    field_ranks  = [fr.get(pref, np.nan),  fr.get(other, np.nan)]
    global_ranks = [gr.get(pref, np.nan),  gr.get(other, np.nan)]
    jif_ranks    = [venue_citedness.get(pref, np.nan),
                    venue_citedness.get(other, np.nan)]

    if np.all(~np.isnan(field_ranks)) and np.all(~np.isnan(jif_ranks)):
        field_correct  = field_ranks[0]  > field_ranks[1]
        jif_correct    = jif_ranks[0]    > jif_ranks[1]
        global_correct = global_ranks[0] > global_ranks[1]

        predictions.append((
            row["field"],
            global_correct,
            field_correct,
            jif_correct,
            field_correct == False and jif_correct == False,
            field_correct == True  and jif_correct == True,
            field_correct == False and jif_correct == True,
            field_correct == True  and jif_correct == False,
        ))

predictions = pd.DataFrame(
    predictions,
    columns=["field", "global_acc", "field_acc", "jif_acc",
             "match_wrong", "match_right",
             "jif_right_field_wrong", "field_right_jif_wrong"],
)

# ── Summary stats ─────────────────────────────────────────────────────────────
summary = predictions.groupby("field").mean()
summary = 100 * summary.round(4)
summary["field - jif"] = summary["field_acc"] - summary["jif_acc"]
summary["agree"]    = summary["match_right"]          + summary["match_wrong"]
summary["disagree"] = summary["jif_right_field_wrong"] + summary["field_right_jif_wrong"]

# ── Plot (cell b6d5b8d7) ──────────────────────────────────────────────────────
plt.rcParams["font.size"]    = 13
plt.rcParams["legend.fontsize"] = 13

summary_sorted = summary.sort_values(by="field_acc", ascending=True)

fig, ax = plt.subplots(figsize=(8, 6))

fields      = summary_sorted.index
y_positions = range(len(fields))

ax.scatter(summary_sorted["field_acc"], y_positions,
           color="k", label="Field consensus\nranking", s=100)
ax.scatter(summary_sorted["jif_acc"], y_positions,
           color="white", alpha=1, s=100, marker="s")
ax.scatter(summary_sorted["jif_acc"], y_positions,
           color="grey", label="JIF Ranking", alpha=0.7, s=100)

# Academia row at the top
x1 = 100 * predictions["global_acc"].mean()
x0 = 100 * predictions["jif_acc"].mean()
y  = len(fields) + 0.5

ax.scatter(x1, y, color="k",     s=100)
ax.scatter(x0, y, color="white", s=100, alpha=1)
ax.scatter(x0, y, color="grey",  s=100, alpha=0.7)
ax.text((x1 + x0) / 2, y,
        f"{abs(x1 - x0):.2f}",
        color="black", ha="center", va="bottom")
ax.plot([x1, x0], [y, y], color="black", linestyle="-", alpha=0.5, zorder=-1)

# Per-field connecting lines + difference labels
for i, field in enumerate(fields):
    field_acc = summary_sorted.loc[field, "field_acc"]
    jif_acc   = summary_sorted.loc[field, "jif_acc"]
    ax.plot([field_acc, jif_acc], [i, i],
            color="black", linestyle="-", alpha=0.5, zorder=-1)
    diff = abs(field_acc - jif_acc)
    ax.text((field_acc + jif_acc) / 2, i,
            f"{diff:.2f}", color="black", ha="center", va="bottom")

# Y-axis
ax.set_yticks(list(y_positions) + [len(fields) + 0.5])
ax.set_yticklabels(list(fields) + ["Academia"])
ax.set_ylabel("Field")
ax.set_xlabel("% of Pairwise Comparisons Correctly Predicted")
ax.legend(loc="lower right", frameon=False)

# White patch to cover the gap between fields and Academia row
plt.text(52.5, len(fields) - 0.25, "   ",
         fontsize=8, va="center", ha="left",
         color="black", backgroundcolor="white", zorder=10)

plt.tight_layout()
sns.despine()

out = HERE / "si_fig3.pdf"
plt.savefig(out, bbox_inches="tight")
print(f"Saved: {out}")
